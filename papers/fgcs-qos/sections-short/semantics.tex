The Semantic Web area, coined by Tim Berners-Lee in 2001, has experienced recent times a growing 
commitment from both academia and industrial areas  with the objective of elevating the meaning of web 
information resources through a common and shared data model (graphs) and an underlying semantics based 
on different logic formalisms (ontologies). The Resource Description Framework (RDF), based on a graph model, and the Web Ontology Language (OWL), 
designed to formalize and model domain knowledge, are the two main \textit{ingredients} to reuse information and data 
in a knowledge-based realm. Thus data, information and knowledge can be easily shared, exchanged and linked~\cite{Maali_Cyganiak_2011} 
to other knowledge-based systems and databases through the use URIs, more specifically HTTP-URIs. Therefore the broad objective of this effort can be summarized 
as a new environment of added-value information and services that can boost and improve B2B (Business to Business), B2C (Business to Client) or 
A2A (Administration to Administration) relationships. The implementation of new context-awareness expert systems to tackle existing 
cross-domain problems such as medical reasoning, analysis of social media, etc. in which data heterogeneities, 
lack of standard knowledge representation and interoperability problems are common factors that the use of semantics can improve. As a practical view of the Semantic Web, 
the Linked Data~\cite{Heath_Bizer_2011} initiative emerges to create a large and distributed database on the Web. 
In order to reach this major objective the publication of information and data under a common data model (RDF) 
with a specific formal query language (SPARQL) provides the required building blocks to turn the Web of documents 
into a real database or ``Web of Data''. Research works are focused in two main areas: 1) production/publishing~\cite{bizer07how} and 2) consumption of 
Linked Data. In the first case data quality~\cite{Bizer2009QA}, conformance~\cite{DBLP:journals/ws/HoganUHCPD12}, 
provenance~\cite{w3c-prov} and trust, description of datasets~\cite{void} and entity reconciliation~\cite{Maali_Cyganiak_2011} issues are becoming major objectives since a mass of amount data is already available through RDF repositories and SPARQL endpoints. 

On the other hand, consumption of Linked Data is being addressed to provide new ways of data 
visualization~\cite{DBLP:journals/semweb/DadzieR11}, faceted browsing~\cite{citeulike:8529753}, 
searching~\cite{hoga-etal-2011-swse-JWS} and data exploitation~\cite{Harth:2011:SIP:1963192.1963318}. Some approaches 
based on sensors~\cite{Jeung:2010:EMM:1850003.1850235}, distributed queries\cite{Hartig09executingsparql}, 
scalable reasoning processes~\cite{DBLP:journals/ws/UrbaniKMHB12}, annotation of web pages~\cite{rdfa-primer} or information retrieval~\cite{Pound} are key-enablers for easing the access 
to information and data. Currently there is a growing commitment to publish a vast amount of existing statistical data due to 
the promotion of existing ``On-Line Analytical Processing'' (OLAP) and ``OnLine Transaction Processing'' OLTP systems. 
In this sense, the ``RDF Data Cube Vocabulary'' a W3C Working Draft document, is a shared effort to 
represent statistical data in RDF reusing parts (the cube model) of the ``Statistical Data and Metadata Exchange Vocabulary'' (SDMX)~\cite{sdmx}, an ISO standard 
for exchanging and sharing statistical data and meta-data among organizations. The Data Cube vocabulary is a core 
foundation which supports extension vocabularies to enable publication of other aspects of statistical data flows or 
other multi-dimensional data sets. Previously, the ``Statistical Core Vocabulary'' (SCOVO) was the standard in 
fact to describe statistical information in the Web of Data. Some works are also emerging to mainly publish statistical data 
following the concepts of the LOD initiative covering statistical analysis of linked data~\cite{DBLP:conf/semweb/ZapilkoM11}, 
statistical data publication~\cite{DBLP:journals/ijsc/SalasMBCMA12}, survey data publication~\cite{DDI2013,DBLP:conf/dgo/FernandezMG11} or 
quantitative indexes structure and metadata~\cite{webindexlod} among others.

All the aforementioned works must be considered in order to re-use existing vocabularies and datasets to address 
the challenges of creating meta-described data, information and knowledge. Mainly semantics allows us to model logical restrictions 
on data while linked data enables the publication of new data and information under a set of principles 
to boost their re-use and automatic processing through machine-readable formats and access protocols with the aim of boosting 
a new wave of professionals~\cite{DBLP:journals/ijhcitp/PalaciosSAG12}.


% Recent times have seen the deployment of the Open Data initiative due to the strategy 
% developed by the governments in USA, Australia, UK or Europe and that have been followed by most of countries around the world to deliver data 
% catalogues containing valuable public sector information (PSI). In this context the Open (Government) Data movement (W3C) is making a great effort 
% to spread this view in public and private bodies with the objective of boosting transparency and a new data-driven economy. 
% From a corporate strategy point of view the re-use of existing open data must encourage and improve 
% more efficient policy-making processes. Under this view there is a perfect matching between the Linked Data and the Open Data 
% principles: on the one hand the Linked Data community is generating the proper environment of standard 
% specifications and tools to manage data and, on the other hand, the Open Data initiative is requiring 
% the right methods to generate an authentic re-use of public data. The combination of these 
% two views leads us to the Linked Open Data (LOD) effort that seeks for applying the principles of Linked Data to implement the Open Data mission.
% 
% Currently one of the mainstreams in the Semantic Web area lies in the application of the LOD initiative in 
% different domains such as  e-Government, e-Procurement, e-Health, Biomedicine, Education, Bibliography or Geography to name a few,  
% with the aim of solving existing problems of integration and interoperability among applications and create a 
% knowledge environment under the Web-based protocols. In this context, the present work is therefore focused 
% in applying semantic web vocabularies and datasets to model quantitative indexes from both structural 
% and computational points of view in a ``Policy-Making'' context. 



% In this context the creation and use of quantitative indexes is a widely accepted practice that has been applied to various 
% domains such as Bibliometrics and academic performance and quality (the Impact Factor by Thomson-Reuters, the H-index or the Shanghai and Webometrics rankings), 
% the Web impact (the Webindex by the Webfoundation) or Cloud Computing (the Service Measurement Index by the CSMIC consortium, the Global Cloud Index by Cisco, 
% the CSC index, the VMWare Cloud Index, etc.) or Smart Cities (The European Smart Cities ranking) to name a few (apart from the traditional ones such as the Gross domestic product). 
% Therefore policymakers as well as individuals are continuously evaluating quantitative measures to tackle or improve 
% existing problems in different areas and support their decisions. Nevertheless the sheer mass of data now available in the web is 
% raising a new dynamic and challenging environment in which traditional tools are facing major 
% problems to deal with data-sources diversity, structural issues or complex processes of estimation. According to some efforts 
% such as the ``Policy-making $2.0$'' within the Cross-Over project~\footnote{\url{http://www.crossover-project.eu/}} that \textit{refers to a blend of emerging and fast developing technologies 
% that enable better, more timely and more participated decision-making}, new paradigms and tools are required to take advantage of 
% the existing environment (open data and big data) to design and estimate actions in this dynamic context according to requirements of 
% transparency, standardization, adaptability and extensibility among others with the aim of providing new context-aware 
% and added-value services such as visualization that can help a deepen and broaden understanding of the impact of a 
% policy in a more fast and efficient way. As a consequence common features and requirements can be extracted from the existing situation out:
% \begin{itemize}
%  \item Data sources. Data and information is continuously being generated as observations from social networks, public and private institutions, NGOs, services and applications, etc. 
%  creating a tangled environment of sources, formats and access protocols with a huge but restricted potential for exploitation. Nevertheless data processing, knowledge inferring, etc. are not mere processes 
%  of gathering and analyzing, it is necessary to deal with semantic and syntactic issues, e.g. particular measurements and dimensions or name mismatches, 
%  in order to enable a proper data/information re-use and knowledge generation.
%  
%  \item Structure. Quantitative indexes are usually defined (a mathematical model) by experts to aggregate several indicators (in a hierarchy structure) in just one value to provide
%  a measure of the impact or performance of some policy in a certain context. The structure of these indexes are obviously subjected to change over time 
%  to collect more information or adjust their composition and relationships (narrower/broader). That is why technology should be able to afford 
%  adequate techniques to automatically populate new changes in an efficient way.
%  
%   \item Computation process. This feature refers to the calculation of the index. Observations are gathered from diverse data sources and aligned 
%   to the index structure, commonly indicators, that are processed through various mathematical operators to generate a final index value. 
%   Nevertheless the computation process is not always described neither open (any minor change can imply a long time for validation) implying that 
%   cannot be easily replied for third-parties with other purposes, for instance research, preventing one 
%   of the most wanted characteristics such as transparency. Furthermore it is necessary to ensure that the computation process 
%   is sound and correct.
% 
%   \item Documentation. As the European project Cross-over has stated, new policy-making strategies go ahead of a simple and closed value and it is necessary to bring 
%   new ways of exploiting data and information. Moreover the use of the Web as a dissemination channel represents a powerful environment in which 
%   information should be available taking into account the multilingual and multicultural character of information. In this context documentation mechanisms 
%   must necessarily cover all the aforementioned features to afford a detailed explanation of a quantitative index-based policy to both policymakers 
%   and final users. However existing initiatives usually generates some kind of hand-made report which is not easy to keep up-to-date and deliver 
%   to the long-tail of interested third-parties.
% \end{itemize}
% 
% On the other hand, the Semantic Web area has experienced during last years a growing commitment from both academia and industrial areas 
% with the objective of elevating the meaning of web information resources through a common and shared data model (graphs) and 
% an underlying semantics based on a logic formalism (ontologies). The Resource Description Framework (RDF), based on a graph model, 
% and the Web Ontology Language (OWL), designed to formalize and model domain knowledge, are a \textit{lingua-franca} to re-use information 
% and data in a knowledge-based environment. Thus data, information and knowledge can be easily shared, exchanged and linked~\cite{Maali_Cyganiak_2011} 
% to other databases through the use URIs, more specifically HTTP-URIs. Therefore the broad objective of this effort can 
% be summarized as a new environment of data-based services to encourage and improve B2B (Business to Business), B2C (Business to Client) or 
% A2A (Administration to Administration) relationships. Under this view the implementation of new context-awareness expert systems 
% to tackle existing cross-domain problems in which data heterogeneities, lack of standard knowledge representation and 
% interoperability problems are common scenarios for applying this approach. Furthermore recent times have also seen the deployment of 
% the Linked (Open) Data~\cite{Berners-Lee-2006,Heath_Bizer_2011} initiative  to make it possible the view and application of the Semantic Web to create a large and distributed database on the Web. 
% 

