Recent times have seen the emergence of new applications to deal with ``Big Data'' that usually 
includes the processing of large datasets and vast amounts of data coming from different sources 
with the objective of extracting ``the most of data'' to support decision processes. These tools 
are focuses on capturing, curating, managing and processing data in a certain slot of time. Due 
to the fact data is continuosly generated from users, services or devices the size of the dataset 
to be processed goes from dozen of terabytes to many petabytes. In this new environment 
traditional Database Management Systems (DBMS) are facing a major challenge to deal with 
a new dynammic and growing data context and new movements such as NoSQL systems are emerging due to 
their ability to handle larger amounts of data in a smart fashion.

The main characteristic of Big Data tools lies in its capability to tackle three (Vs) main dimensions: 
1) volume (amount of data), 2) velocity (speed of data in and out) and 3) variety (range of 
data types and sources). In this sense Gartner has established a Big Data definition 
that perfectly summarizes what a Big Data system is: ``Big data are high volume,
high velocity, and/or high variety information assets that require new forms of
processing to enable enhanced decision making, insight discovery and process
optimization.''. Nevertheless a new ``V'' (``Veracity'') has been added in 
some organizations to assess the quality of results in Big Data systems. At a first glance 
a big difference with the Business Intelligence community is hard to draw but the maturity 
of Big Data tools makes this difference more obvious: Business Intelligence uses 
descriptive statistics and high-density information while Big Data is focused on low-density but large volumes of data and 
inductive statistics e.g. regression models to predict some variable.

Therefore systems~\cite{BigDataComputing} that require real-time, search or high-frequency trading 
in a certain context such as smart cities, advertising or social networks are moving 
to this kind of Big Data architectures to be able to process large 
volumes of data in highly scalable and streaming fashion. Existing tools 
and frameworks use or implement a streaming strategy of partitioning 
the input data into fixed-size segments as MapReduce-based frameworks do but
the main drawback of this approach lies in the latency (it is proportional to
the length of the segment plus the overhead required to do the segmentation and initiate
the processing jobs). In this case the size of the segment is a key-decision 
to get an optimal data-processing system. Nevertheless new architectures such 
as the ``Lambda Architecture''~\cite{BigDataManing} minimizes this issue adding different layers 
of processing to operate with data streams in real-time.

The evolving Big Data Community is unleashing the potential of these tools 
to drive innovation through the creation of new platforms with more 
and more analysis capabilities that try to fulfill both market 
and research areas. Forrester has outlined the importance of 
this new rise of big data as an opportunity to increase their knowledge 
and get competitive advantages with regards to their competitors making 
better decisions. In this sense it seems clear that the use of 
predictive analytics to find patterns in data represents a new 
market of opportunities and a real development of a 
new data-based economy.


% 
% In order to maximize success with predictive analytics programs, organizations must (see Figure 1):
% 
% Set the business goals. Clearly stated business goals lie at the center of any successful predictive
% analytics project. For example, the goal might be to recommend items to upsell to existing
% customers — or to prevent life-threatening and costly hospital re-admittance. Businesses can
% also use predictive analytics to achieve more generic business goals, such as increasing revenue,
% because it enables them to discover correlations that may suggest strategy improvements.
% 
% 
% 	 Understand data from a variety of sources. In large organizations, potentially valuable data
% often exists in multiple siloes. In addition, many firms are now using external data from social
% media, government data, and other public sources of data to augment their internal data.
% Advanced data visualization tools can help data analysts explore the data from various sources
% to determine what might be relevant for a predictive analytics project. Increasingly, many data
% analysts collect every shred of data available to let the predictive analysis algorithms find what is
% most relevant.
% Prepare the data. Data preparation for predictive analysis is a key challenge.3 Raw data is often
% unsuitable for predictive analytics. Data analysts must often perform extensive preprocessing
% of the data before running analysis algorithms. For example, data analysts might need to enrich
% the data with calculated aggregate fields, strip out extraneous characters or information that
% would choke the algorithms, or combine data from multiple sources.
% 
% Create the predictive model. Data analysts use predictive analytics modeling tools to run
% analysis algorithms against the data. There are hundreds of different statistical and machine
% learning algorithms and combinations that data analysts can run against the data to find
% predictive models. Data analysts typically run the analysis on a subset of the data called
% “training data” and set aside “test data” that they will use to evaluate the model. For example,
% data analysts may run the algorithms on a training data set that is 70% of the entire data set;
% they will then use the remaining 30% as the test data set to evaluate the predictive model.
% 
% 	 Evaluate the model. Predictive analytics is not about absolutes; it is about probabilities. To
% evaluate the predictive power of the model, data analysts run it against the test data set. If the
% predictive model is more effective than a random selection of the outcome, then they’ve found
% an effective predictive model. Data analysts can continue to run other types of algorithms until
% they find the one that is most predictive; alternatively, they may not find any because there
% is not enough data or the data is too random to uncover a predictive model for the desired
% business outcome.
% ■	 Deploy the model. Analysts must deploy effective predictive models in production applications
% to accrue the business benefits. A deployed model consists of logic to run the predictive rules
% and/or formulas and a method to get the data that the model needs and return the result.
% ■	 Monitor the effectiveness of the model. As financial firms caution, “Past results do not
% guarantee future performance.” It is essential to monitor the effectiveness of the predictive
% model. For example, if mobile carrier A starts to offer a free data plan, then the reasons why
% customers churn from carrier B can change. Firms must continue the predictive analytics
% process to stay on top of business goals, understand new data, prepare better data, refine models
% with new algorithms, evaluate the models, and deploy and monitor the models in a never-
% ending cycle.
% 
% 
% he vendors evaluated in this Forrester Wave provide general purpose big data predictive analytics
% solutions to facilitate the predictive analytics process and ease the burden of this never-ending,
% continuous cycle of model discovery, deployment, and optimization that can be applied to most
% industries and business domains. In addition to the general purpose solutions evaluated in this
% Forrester Wave, firms that wish to benefit from big data predictive analytics solutions can also
% choose among:
% ■	 Vertical or horizontal solutions. Many vendors provide solutions that focus on specific
% industry or horizontal domains, such as customer analytics. For example, Forrester has
% evaluated solutions offered by Fair Isaac (FICO) and Pitney Bowes that specifically focus on
% customer-focused programs and initiatives that drive acquisition, retention, cross-sell/upsell,
% and targeted marketing campaigns.4 Other examples of vertical solutions include cloud-based
% offerings such as BloomReach, which uses predictive analytics to help eCommerce companies
% sell more online by showing customers more relevant content, and startup company Objective
% Logistics, which uses big data predictive analytics to help restaurants increase sales by
% improving workforce planning.
% 
% 
% Embedded solutions. Other platforms increasingly embed predictive analytics capabilities.
% BI platforms such as Alteryx and Pentaho include embedded predictive analytics features
% in addition to BI functionality. Business process management (BPM) platforms such as
% Pegasystems and Rage Frameworks also offer predictive analytics capabilities.
% ■	 Database analytics. Relational database management systems (RDBMS), EDWs, NoSQL,
% Hadoop, and other data-focused hardware and software have some predictive analytics
% capabilities, but they tend to be oriented toward technical users and require programming or
% SQL. For example, Teradata’s Aster provides a big data predictive analytics capability that allows
% developers to use SQL and MapReduce together to perform sophisticated analysis on very
% large data sets. Similarly, programmers can use open source machine learning libraries such as
% Apache Mahout for Hadoop or a Java library such as Weka for predictive analytics.
% ■	 Offerings from consulting firms. Enterprises that lack expertise in predictive analytics or that
% wish to outsource can choose from among many mainstream or boutique consulting firms that
% focus on predictive analytics. For example, Opera Solutions is a consulting firm that creates
% predictive models that focus on specific business outcomes. These firms will often use general
% purpose solutions such as those evaluated in this Forrester Wave, but they also provide deep
% knowledge and expertise in analyzing data and creating predictive models.
% 
% 
% % 
% % Evaluation Criteria: Current Offering, Strategy, And Market Presence
% % After examining past research, user need assessments, and vendor and expert interviews, we
% % developed a comprehensive set of evaluation criteria. We evaluated vendors against 51 criteria,
% % which we grouped into three high-level buckets:
% 
% 
% 
% 
% 
